{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to preprocess features to be between 0-1\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_cancer, y_cancer = dataset.data, dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "# normalizing the data for more accurate results\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# an MLP classifier is sklearn's version of a neural network. A neural network is an ensemble of nodes\n",
    "# connected to other nodes sequentially. Each node in the network receives some input and intern\n",
    "# will give some output. Nodes are connected by their corresponding weights. Each node has it's own\n",
    "# activation function (something that determines whether the node is excited or not) \n",
    "# and each layer has it's own bias (similar to weight).\n",
    "\n",
    "# lets take a simple example of a neural network that has 2 layers of 10 nodes each.\n",
    "# As an example project, we will want the neural network to predict which number a certain\n",
    "# array is representing based on it's index.\n",
    "# eg. given [0,0,1,0,0,0,0,0,0,0] the network should predict 2 as the highest value is in the second\n",
    "# position.\n",
    "\n",
    "# before we begin, the network sets the weight values and the bias value randomly. This means that\n",
    "# the connection between the nodes is random. Lets say that the first data the network sees is the\n",
    "# following: [1,0,0,0,0,0,0,0,0,0]. We would want the output nodes to show [1,0,0,0,0,0,0,0,0,0]\n",
    "# as the output but you would probably get something that looks like this:\n",
    "# [-1, 43, 6.4, 3, ...., 0.3] instead. This is because the weights and the bias are currently set to\n",
    "# be random values.\n",
    "\n",
    "# So how does the network correct itself? It takes advantage of two key things: an optimizer and\n",
    "# a loss function. The loss function says how far off from the ideal value the output is, and the\n",
    "# optimizer shifts the weights so that it can make the loss function have a smaller value. After\n",
    "# repeatedly correcting itself through each run through, you will arrive at a more accurate estimation.\n",
    "\n",
    "# it is important to remember the role that activation functions play in neural networks. Activation\n",
    "# functions are assigned to each node individually and help to determine what the output of the node\n",
    "# will be. Common activation functions are:\n",
    "# relu - set output to 0 if input <= 0 otherwise output = input\n",
    "# sigmoid - squish value between 0-1\n",
    "# tanh - squish value between -1 and +1\n",
    "\n",
    "# here we are saying that we want a neural network with two layers both having 100 nodes. We also\n",
    "# want to use L2 regularization with an alpha value of 5.0 so that our model does not overfit. Finally,\n",
    "# we are specifying that we want the lbfgs optimizer.\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=[100,100], \n",
    "    alpha=5.0, \n",
    "    random_state=0, \n",
    "    solver='lbfgs'\n",
    ").fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of NN classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of NN classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making random regression data\n",
    "plt.figure()\n",
    "plt.title('Sample regression')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(2, 3, figsize=(15, 12))\n",
    "\n",
    "# create a list of 50 lists that have a single value between -3 and 3\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "\n",
    "# tricky list indexing here. I am saying traverse the whole list, but go up by 5 each time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state=0)\n",
    "\n",
    "# through this traversal, we want to test the different activation functions on each row\n",
    "# and the different alpha values on each column and see how the change affects the output\n",
    "for axrow, actrow in zip(subaxes, ['tanh', 'relu']):\n",
    "    for alpcol, ax in zip([0.0001, 1.0, 100], axrow):\n",
    "        # using the regression version of the neural network here. This is similar in architecture\n",
    "        # to the classifier but the output will be a continuous value\n",
    "        mlpreg = MLPRegressor(\n",
    "            hidden_layer_sizes = [100,100],\n",
    "            activation = actrow,\n",
    "            alpha = alpcol,\n",
    "            solver = 'lbfgs'\n",
    "        ).fit(X_train, y_train)\n",
    "        # get prediction values from generated input values\n",
    "        y_predict_output = mlpreg.predict(X_predict_input)\n",
    "        # plot generated input vs prediction\n",
    "        ax.plot(X_predict_input, y_predict_output, '^', markersize = 10)\n",
    "        # plot input vs true output\n",
    "        ax.plot(X_train, y_train, 'o')\n",
    "        ax.set_xlabel('Input feature')\n",
    "        ax.set_ylabel('Target value')\n",
    "        ax.set_title('MLP regression\\nalpha={}, activation={})'.format(alpcol, actrow))\n",
    "        plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
