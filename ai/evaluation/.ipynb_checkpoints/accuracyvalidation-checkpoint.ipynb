{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads built in hand written digits data set\n",
    "dataset = load_digits()\n",
    "# breaking data set into features and labels\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# showing how many of each class there are in the data set\n",
    "# note that np.bincount counts the number of occurances of each value in\n",
    "# an array of non-negative ints\n",
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the previous distribution shows that each class is pretty much uniformly distributed\n",
    "\n",
    "# here we are creating a distribution where the only positive classes are the ones whos label\n",
    "# is 1\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print(\"original labels: \", y[1:30])\n",
    "print(\"new binary labels: \", y_binary_imbalanced[1:30])\n",
    "# this shows us that there are 1615 negative classes and only 182 positive classes therefore\n",
    "# we have a class imbalance\n",
    "print(\"class balance: \", np.bincount(y_binary_imbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "\n",
    "# This accuracy seems good, but we will check if this accuracy is truly indicative of a good model\n",
    "# using comparisons between this model and dummy classifiers\n",
    "print(\"model accuracy: \", svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this classifiers strategy is to always predict the most frequently occuring class.\n",
    "# In our data set, that would be the negative class\n",
    "\n",
    "# besides most_frequent, we could use one of the following parameters:\n",
    "# stratified: random predictions based on test set distributions\n",
    "# uniform: generates predictions uniformly at random\n",
    "# constant: predicts a constant label provided by the user\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train) \n",
    "\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "print(\"output of dummy classifiers predictions\")\n",
    "print(y_dummy_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although this model only predicts the negative class for each instance, it still is\n",
    "# calculated to have an accuracy of 90%. This means that accuracy is not always the\n",
    "# best indicator that your model is of high quality\n",
    "print(\"accuracy of dummy classifier: \", dummy_majority.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_majority = DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train)\n",
    "\n",
    "y_majority_predicted = dummy_majority.predict(X_test)\n",
    "\n",
    "# a confusion matrix displays how many true positive, false positives, false negatives, and true negatives\n",
    "# there are in a certain prediction set\n",
    "confusion = confusion_matrix(y_test, y_majority_predicted)\n",
    "\n",
    "# the table is arranged like this:\n",
    "# true negative          false positive\n",
    "# false negative         true positive\n",
    "# correct predictions are represented on the diaganol while errors are represented off the diagonal\n",
    "print(\"most frequent class prediction (dummy classifier)\\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classprop = DummyClassifier(strategy=\"stratified\").fit(X_train, y_train)\n",
    "\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_classprop_predicted)\n",
    "\n",
    "# here we can see that since we used the stratified strategyy instead of the most frequent\n",
    "# strategy, we have some values coming up in the false and true positive sections\n",
    "print(\"random class-proportional predictions (dummy classifier)\\n\", confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
