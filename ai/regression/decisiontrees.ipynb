{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in data set to sklearn\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=3)\n",
    "\n",
    "# a decision tree is a type of supervised learning algorithm where a tree is constructed\n",
    "# using rules that is learned from the data set. At the root of the tree, you would have\n",
    "# the entire population and at the leaves of the tree, you would have the class label. \n",
    "# Each internal node represents a test on an attribute and each branch indicates the outcome\n",
    "# of the test.\n",
    "\n",
    "# in this example, we are classifying 3 different types of iris plants based on a number of features.\n",
    "# One split at the top for example may be septal width. Because all 3 types of iris have different\n",
    "# septal widths, we could split the data into two different sub sections where plants that had a \n",
    "# septal width of < 3cm would be a certain type of plant and the other two would have > 3cm. This\n",
    "# process is continued until we have very specific feature splits and we arrive at the leaf nodes\n",
    "# which will have class labels associated with them\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"accuracy of decision tree (train): {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"accuracy of decision tree (test): {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have seen that the previous model was overfitting. this was due to the fact that\n",
    "# the model essentially memorized the training data by having pure leaf nodes at the bottom.\n",
    "# This would lead to having very small ranges of classification for unseen values\n",
    "\n",
    "# in order to combat the overfitting, we can try and limit the tree depth to be 3.\n",
    "# also note that we can change the min_samples_leaf and max_leaf_nodes parameters as well.\n",
    "# the min_samples_leaf parameter details the minimum number of samples a node must have\n",
    "# in order to be split. The max_leaf_nodes parameter limits the max number of leaf nodes\n",
    "# that the tree can have\n",
    "clf2 = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "print(\"accuracy of decision tree (train): {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"accuracy of decision tree (test): {}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "# feature importance explains how crucial a certain feature is when classifying it. A\n",
    "# value closer to 1 signifies a higher importance while a value closer to zero signifies\n",
    "# a lower importance\n",
    "print(\"feature importances: {}\".format(clf.feature_importances_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
